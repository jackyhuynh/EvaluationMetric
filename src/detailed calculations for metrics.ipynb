{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2afbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a443a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed and test fraction declaration\n",
    "random_seed = 25\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "test_data_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683ceb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the data and put into a dataframe\n",
    "bc_sk = datasets.load_breast_cancer()\n",
    "bc_sk.data = MinMaxScaler().fit_transform(bc_sk.data)\n",
    "bc_data = pd.DataFrame(data= np.c_[bc_sk['data'], bc_sk['target']],columns= list(bc_sk['feature_names'])+['target'])\n",
    "bc_features = bc_data.iloc[:,0:-1]\n",
    "bc_labels = bc_data[\"target\"]\n",
    "\n",
    "# split the data into test and train and create a decision tree\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(bc_features, bc_labels, test_size=test_data_fraction,  random_state=random_seed)\n",
    "Y_test_predicted = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed).fit(X=X_train, y=Y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae6d18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33,  6],\n",
       "       [ 3, 72]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the confusion matrix on the test data\n",
    "confusion_matrix(Y_test, Y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47eb3392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in Y_test_predicted =  114\n",
      "Total records in Y_test_predicted = 0 = 33+3 =  36\n",
      "Total records in Y_Test =  114\n",
      "Total records in Y_Test = 0 = 33 + 6 =  39 \n",
      "\n",
      "Accuracy = (33+72)/114 =  0.9210526315789473\n",
      "Accuracy: 0.9210526315789473\n",
      "\n",
      "Precision Macro for label 0 = 33/(33+3) =  0.9166666666666666\n",
      "Precision Macro for label 1 = 72/(6+72) =  0.9230769230769231\n",
      "Precision Macro average = (33/(33+3) + 72/(6+72))/2 =  0.9198717948717949\n",
      "Precision Macro: 0.9198717948717949 \n",
      "\n",
      "Recal Macro for label 0 = 33/(33+6) =  0.8461538461538461\n",
      "Recal Macro for label 1 = 72/(3+72) =  0.96\n",
      "Recal Macro average = (33/(33+6) + 72/(3+72))/2 =  0.9030769230769231\n",
      "Recall Macro: 0.9030769230769231 \n",
      "\n",
      "F1 Macro for label 0 = 2*33/(2*33 + 6 + 3) 0.88\n",
      "F1 Macro for label 1 = 2*72/(2*72 + 6 + 3) 0.9411764705882353\n",
      "F1 Macro average = (2*33/(2*33 + 6 + 3) + 2*72/(2*72 + 6 + 3))/2 =  0.9105882352941177\n",
      "F1 Macro: 0.9105882352941176\n",
      "\n",
      "Weighted average for precision = .9167*39/114 + .9231*75/114 =  0.9209105263157895\n",
      "Weighted average for recal = .8462*39/114 + .9600*75/114 =  0.9210684210526314\n",
      "Weighted average for precision = .8800*39/114 + .9412*75/114 =  0.9202631578947369\n"
     ]
    }
   ],
   "source": [
    "# this is where the support numbers come from\n",
    "print(\"Total records in Y_test_predicted = \", len(Y_test_predicted))\n",
    "print(\"Total records in Y_test_predicted = 0 = 33+3 = \", 33+3)\n",
    "print(\"Total records in Y_Test = \", len(Y_test))\n",
    "print(\"Total records in Y_Test = 0 = 33 + 6 = \", 33+6, '\\n')\n",
    "\n",
    "# this is how the accuracy was calculated\n",
    "print(\"Accuracy = (33+72)/114 = \", (33+72)/114)\n",
    "print(f'Accuracy: {sklearn.metrics.accuracy_score(Y_test, Y_test_predicted)}' + '\\n')\n",
    "\n",
    "# this is how the macro precision is calculated\n",
    "print(\"Precision Macro for label 0 = 33/(33+3) = \", 33/(33+3))\n",
    "print(\"Precision Macro for label 1 = 72/(6+72) = \", 72/(6+72))\n",
    "print(\"Precision Macro average = (33/(33+3) + 72/(6+72))/2 = \", (33/(33+3) + 72/(6+72))/2)\n",
    "print(f'Precision Macro: {sklearn.metrics.precision_score(Y_test, Y_test_predicted, average=\"macro\")}', '\\n')\n",
    "\n",
    "# this is how the recal macro is calculated\n",
    "print(\"Recal Macro for label 0 = 33/(33+6) = \", 33/(33+6))\n",
    "print(\"Recal Macro for label 1 = 72/(3+72) = \", 72/(3+72))\n",
    "print(\"Recal Macro average = (33/(33+6) + 72/(3+72))/2 = \", (33/(33+6) + 72/(3+72))/2)\n",
    "print(f'Recall Macro: {sklearn.metrics.recall_score(Y_test, Y_test_predicted, average=\"macro\")}', '\\n')\n",
    "\n",
    "# This is how the F1 macro is calculated\n",
    "print(\"F1 Macro for label 0 = 2*33/(2*33 + 6 + 3)\", 2*33/(2*33 + 6 + 3)) \n",
    "print(\"F1 Macro for label 1 = 2*72/(2*72 + 6 + 3)\", 2*72/(2*72 + 6 + 3))\n",
    "print(\"F1 Macro average = (2*33/(2*33 + 6 + 3) + 2*72/(2*72 + 6 + 3))/2 = \", (2*33/(2*33 + 6 + 3) + 2*72/(2*72 + 6 + 3))/2)\n",
    "print(f'F1 Macro: { sklearn.metrics.f1_score(Y_test, Y_test_predicted, average=\"macro\") }' + '\\n')\n",
    "\n",
    "print(\"Weighted average for precision = .9167*39/114 + .9231*75/114 = \", .9167*39/114 + .9231*75/114)\n",
    "print(\"Weighted average for recal = .8462*39/114 + .9600*75/114 = \", .8462*39/114 + .9600*75/114)\n",
    "print(\"Weighted average for precision = .8800*39/114 + .9412*75/114 = \", .8800*39/114 + .9412*75/114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c714f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9167    0.8462    0.8800        39\n",
      "         1.0     0.9231    0.9600    0.9412        75\n",
      "\n",
      "    accuracy                         0.9211       114\n",
      "   macro avg     0.9199    0.9031    0.9106       114\n",
      "weighted avg     0.9209    0.9211    0.9202       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree\")\n",
    "print(classification_report(Y_test,Y_test_predicted,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a564ed35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33  6]\n",
      " [ 3 72]] \n",
      "\n",
      "Precision Micro = (33+72)/(33+72+3+6) =  0.9210526315789473\n",
      "Precision Micro: 0.9210526315789473 \n",
      "\n",
      "Recal Micro = (33+72)/(33+72+3+6) =  0.9210526315789473\n",
      "Recall Micro: 0.9210526315789473 \n",
      "\n",
      "F1 Micro = (2*recal*precision) / (recal + precision) =  0.910526315789473\n",
      "F1 Micro: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# the following shows the complete calculations for the micro metrics\n",
    "print(confusion_matrix(Y_test, Y_test_predicted), '\\n')\n",
    "\n",
    "print(\"Precision Micro = (33+72)/(33+72+3+6) = \", (33+72)/(33+72+3+6))\n",
    "print(f'Precision Micro: {sklearn.metrics.precision_score(Y_test, Y_test_predicted, average=\"micro\")}', '\\n')\n",
    "\n",
    "print(\"Recal Micro = (33+72)/(33+72+3+6) = \", (33+72)/(33+72+3+6))\n",
    "print(f'Recall Micro: {sklearn.metrics.recall_score(Y_test, Y_test_predicted, average=\"micro\")}', '\\n')\n",
    "\n",
    "print(\"F1 Micro = (2*recal*precision) / (recal + precision) = \", (2*.910526315789473*.910526315789473)/(.910526315789473+.910526315789473))\n",
    "print(f'F1 Micro: { sklearn.metrics.f1_score(Y_test, Y_test_predicted, average=\"micro\") }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fd977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
