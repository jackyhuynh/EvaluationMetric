{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Workshop 5**\n",
    "\n",
    "In this workshop, you'll looking at evaluation metrics and hyperparameter turning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Loading Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "# we're using the Diabetes dataset from sklearn.datasets\n",
    "from sklearn import datasets\n",
    "# Remember you have to run this cell block before continuing!\n",
    "\n",
    "# set a seed for reproducibility\n",
    "random_seed = 25\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Meet the Metrics (Follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# This is a dummy dataset that contains 500 positive and 500 negative samples\n",
    "X,Y = make_classification(n_samples=1000,n_features=4,flip_y=0,random_state=random_seed)\n",
    "\n",
    "test_data_fraction = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_data_fraction,  random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Y_test_predicted = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed).fit(X=X_train, y=Y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision Macro: 0.9184393588063313\n",
      "Recall Macro: 0.9201336167628302\n",
      "F1 Macro: 0.9191919191919192\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {sklearn.metrics.accuracy_score(Y_test, Y_test_predicted)}')\n",
    "print(f'Precision Macro: {sklearn.metrics.precision_score(Y_test, Y_test_predicted, average=\"macro\")}')\n",
    "print(f'Recall Macro: {sklearn.metrics.recall_score(Y_test, Y_test_predicted, average=\"macro\")}')\n",
    "print(f'F1 Macro: { sklearn.metrics.f1_score(Y_test, Y_test_predicted, average=\"macro\") }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Micro: 0.92\n",
      "Recall Micro: 0.92\n",
      "F1 Micro: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Since the datset is balanced in term of class distribution, all of the micro scores are the same as the accuracy\n",
    "print(f'Precision Micro: {sklearn.metrics.precision_score(Y_test, Y_test_predicted, average=\"micro\")}')\n",
    "print(f'Recall Micro: {sklearn.metrics.recall_score(Y_test, Y_test_predicted, average=\"micro\")}')\n",
    "print(f'F1 Micro: { sklearn.metrics.f1_score(Y_test, Y_test_predicted, average=\"micro\") }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also has a [built in function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) that will give a handy summary of all the popular classification metrics. You can use this for the later questions.\n",
    "\n",
    "Precision, Recall and F1 are reported for **each class separately**. For the \"0\" row, a 0 is treated as the positive class. For the \"1\" row, the 1 is treated as the positive class. This is helpful because Precision and Recall are both sensitive to which class is considered positive. **Support** is the number of instances of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9358    0.9189    0.9273       111\n",
      "           1     0.9011    0.9213    0.9111        89\n",
      "\n",
      "    accuracy                         0.9200       200\n",
      "   macro avg     0.9184    0.9201    0.9192       200\n",
      "weighted avg     0.9203    0.9200    0.9201       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test,Y_test_predicted,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compare some classifiers. Soon you'll learn about the K-nearest-neighbors and Adaboost classifiers. For now, all you need to know is that they're very different appraoches than decision tress, and you should expect them to have different perforamnce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "Y_test_predicted = KNeighborsClassifier(n_neighbors=3).fit(X=X_train, y=Y_train).predict(X_test)\n",
    "print(\"KNN Classifer\")\n",
    "print(classification_report(Y_test,Y_test_predicted,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Y_test_predicted = AdaBoostClassifier(n_estimators=100, random_state=random_seed).fit(X=X_train, y=Y_train).predict(X_test)\n",
    "print(\"Adaboost Classifier\")\n",
    "print(classification_report(Y_test,Y_test_predicted,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *dummy classifier* always picks the majority. We use the to make sure a classifier is doing better than a naive approach that wouldn't require any real training (classifiers don't always do better!).\n",
    "\n",
    "What do the precision, recall and accuracy represent in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dummy Classifier (Picks the majority class. Every time.)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "Y_test_predicted = DummyClassifier(strategy=\"most_frequent\", random_state=random_seed).fit(X=X_train, y=Y_train).predict(X_test)\n",
    "print(\"Dummy Classifier\")\n",
    "print(classification_report(Y_test,Y_test_predicted,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the classifiers. Which is the best? What metric are you using to compare them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Imbalanced data (Group)\n",
    "In this problem, you'll be trying to predict the presence of breast cancer from various features from medical readings. This can help doctors make better diagnoses and save lives.\n",
    "\n",
    "Breast cancer is a common canncer, but relatively rare overall. However, this dataset includes more positive instances (people with breast cancer) than negative. Why might that be the case?\n",
    "\n",
    "In this problem, we'll learn how to deal with these \"imballanced\" datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Read the breast cancer prediction dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "bc_sk = datasets.load_breast_cancer()\n",
    "\n",
    "# Make sure data is in the same range\n",
    "bc_sk.data = MinMaxScaler().fit_transform(bc_sk.data)\n",
    "\n",
    "# Note that the \"target\" attribute is species, represented as an integer\n",
    "bc_data = pd.DataFrame(data= np.c_[bc_sk['data'], bc_sk['target']],columns= list(bc_sk['feature_names'])+['target'])\n",
    "bc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_fraction = 0.2\n",
    "bc_features = bc_data.iloc[:,0:-1]\n",
    "bc_labels = bc_data[\"target\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(bc_features, bc_labels, test_size=test_data_fraction,  random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the ratio of class values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's around a 60/40 split. What effect do you think this will have on the various evaluation metrics? For example, how could a classifier easily get 100% recall, 60% accuracy and 60% precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the evaluation metrics as like above for Decision Trees, KNN, Adaboost, and the Dummy Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these metrics, answer the following questions:\n",
    "\n",
    "1. Which model would you select and why? \n",
    "2. What metric(s) are most important for the breast cancer classification problem?\n",
    "3. How would you recommend a doctor actually use the model **in practice**? Is it good enough to make decisions on its own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Multiclass Data (Group)\n",
    "\n",
    "Now, we'll be looking at the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the iris dataset and translate to pandas dataframe\n",
    "wine_sk = datasets.load_wine()\n",
    "# Note that the \"target\" attribute is species, represented as an integer\n",
    "wine_data = pd.DataFrame(data= np.c_[wine_sk['data'], wine_sk['target']],columns= wine_sk['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# The fraction of data that will be test data\n",
    "test_data_fraction = 0.90\n",
    "\n",
    "wine_features = wine_data.iloc[:,0:-1]\n",
    "wine_labels = wine_data[\"target\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wine_features, wine_labels, test_size=test_data_fraction,  random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are represented as numbers. This is just shorthand to make it easier to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) is useful for getting a broad overview of how your classifier handled certain classes. Below, create a confusion matrix using the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Now create a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documentation for `confusion_matrix`. How many instances were predicted to be class 0 but were actually class 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the evaluation metrics as like above for Decision Trees, KNN, Adaboost, and the Dummy Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions below:\n",
    "\n",
    "1. Which model would you select if you cared equally about each class being correct? \n",
    "2. What if you cared most about accurately detecting Class 0? \n",
    "3. Would you ever choose the Decision Tree model over Adaboost? If so, when? If not, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Cross Validation and Hyperparmeter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Basic Cross Validation (Follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a k-fold splitter\n",
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kf.split() allows you to iterate though the different folds\n",
    "# \"train_index\" are the indecies of the training data in that fold\n",
    "# \"test_index\" are the indicies of the testing data in that fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print(\"Train: \", train_index)\n",
    "    print(\"Test: \", test_index)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Hyperparameter Tuning with CV (Group)\n",
    "\n",
    "We did some very basic HP Tuning last workshop. However, one of the main issues is that we did HP tuning by testing our HPs againt the test dataset. It's good practice not to touch your dataset at all until you've finished selecting your model completly. Therefore, in this exercise we'll be trying out different HPs by constructing validation sets from our training data.\n",
    "\n",
    "The dataset we'll be using for this exercise is the breast cancer dataset, which is used to tell if a certain individal might have breast cancer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Read the wine dataset and translate to pandas dataframe\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "bc_sk = datasets.load_breast_cancer()\n",
    "\n",
    "# Make sure data is in the same range\n",
    "bc_sk.data = MinMaxScaler().fit_transform(bc_sk.data)\n",
    "\n",
    "# Note that the \"target\" attribute is species, represented as an integer\n",
    "bc_data = pd.DataFrame(data= np.c_[bc_sk['data'], bc_sk['target']],columns= list(bc_sk['feature_names'])+['target'])\n",
    "bc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting our data\n",
    "test_data_fraction = 0.2\n",
    "bc_features = bc_data.iloc[:,0:-1]\n",
    "bc_labels = bc_data[\"target\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(bc_features, bc_labels, test_size=test_data_fraction,  random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_accuracy(k, model, X_data, Y_data):\n",
    "    \n",
    "    # Init k-fold splitter\n",
    "    kf = KFold(n_splits=k)\n",
    "    scores = []\n",
    "    \n",
    "    #use kf.split to split the train data into train and validation data\n",
    "    #iterate through all possible folds and fit the folded training data to the model\n",
    "    #use the validation data to predict on the model\n",
    "    #compute the accuracy score and append it to scores\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing K-fold\n",
    "k = 3\n",
    "model = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed)\n",
    "per_fold_acc = k_fold_accuracy(k, model, X_train, Y_train)\n",
    "print(per_fold_acc)\n",
    "np.mean(per_fold_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also exists a built in sklearn function for this, however it is import to know how to perform your own k-fold cross validation split if you want to implement a custom evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# We're using the trianing dataset here, but remember that CV will\n",
    "# split that data into training and validation sets for each fold\n",
    "# so we get an \"unbiased\" estimate of our test performance.\n",
    "per_fold_acc = cross_val_score(model, X_train, Y_train, cv=KFold(n_splits=k), scoring='accuracy')\n",
    "print(per_fold_acc)\n",
    "np.mean(per_fold_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why would we ever do this by hand, if there's already a built-in method?* \n",
    "\n",
    "Sometimes our model training process is more complex than just fitting the model. For example, we may want to do:\n",
    "* Feature selection\n",
    "* Normalization / scaling\n",
    "* More complex models not in the sklearn library\n",
    "\n",
    "In these cases, we can still *only use our training data*! You can't use test data to select features - that would be \"cheating.\" So everying that you use your training data for has to occur within the loop we wrote for CV, above, based on the training data for the particular fold we're evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tuning (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you are going to select the best hypterparameter, using *only the training dataset*. No peaking at the test dataset. To estimate how well a given hyperparameter value will do on *unseen* data, we can use Crossvalidation (within the training dataset) to evaluate our model.\n",
    "\n",
    "Let's use this approach to select the best `ccp_alpha` hyperparameter for a Decision Tree model.\n",
    "\n",
    "You should:\n",
    "1. Iterate over all ccp_alpha values\n",
    "2. Calculate the k_fold validation accuracy using the above funciton\n",
    "3. Calculate the training accuracy and the validation accuracy\n",
    "4. Plot both accuracies vs. the ccp_alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# np.arange generates a list that starts at minimum, ends at maximum, and increments by step\n",
    "alpha_values = np.arange(0, 0.035, 0.002)\n",
    "\n",
    "# two lists to hold our accuracy\n",
    "k = 5\n",
    "valid_accs = []\n",
    "train_accs = []\n",
    "\n",
    "# Put your solution here!\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(alpha_values, valid_accs, color='red')\n",
    "plt.plot(alpha_values, train_accs, color='blue')\n",
    "plt.xlabel(\"Post Pruning Alpha\")\n",
    "plt.ylabel(f'Average Accuracy of {k}-fold validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code selects the alpha value for the best model. Then you job is to train a new model (using all of the training data), using your best hyperparameter value. Then evaluate it on the test dataset. What is the accuracy, precision, recall and F1 Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the alpha for the model with the best accuracy on the *validation* set!\n",
    "best_alpha = alpha_values[np.argmax(valid_accs)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model here. You may want to print the tree using plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate your model on the test dataset.\n",
    "# How does it perform compared your model from the last workshop that didn't use CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has some built in methods for [plotting ROC curves](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Plotting ROC Curves (Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make an ROC curve for the model you selected with HP tuning\n",
    "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed, ccp_alpha=best_alpha).fit(X=X_train, y=Y_train)\n",
    "metrics.plot_roc_curve(gini_tree,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, make an ROC curve with an AdaBoostClassifier with n_estimators=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0).clf()\n",
    "\n",
    "# When predicting, we have to ask for *continuous* values, not 0/1, so we use predict_proba\n",
    "# We use [:,1] to get the predictions for the positive class\n",
    "tree_predictions = gini_tree.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(Y_test, tree_predictions)\n",
    "auc = metrics.roc_auc_score(Y_test, tree_predictions)\n",
    "plt.plot(fpr,tpr,label=\"Decision Tree, auc=\"+str(auc))\n",
    "\n",
    "adaboost_predictions = ada.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(Y_test, adaboost_predictions)\n",
    "auc = metrics.roc_auc_score(Y_test, adaboost_predictions)\n",
    "plt.plot(fpr,tpr,label=\"Adaboost, auc=\"+str(auc))\n",
    "\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Intepreting ROC curves (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the above ROC curves. How are they similar? How do they differ? Is one strictly better than the other? In what situations is one better than the other? Discuss with your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take notes of your discussion here.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
